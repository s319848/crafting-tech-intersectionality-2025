# crafting-tech-intersectionality-2025
This repository contains the code written to carry out the experiments for the project of the course "Crafting Tech", 2025 edition, for the project "AI Intersectionality"

```CT_semantic_analysis.ipynb```

This file contains all the semantic analysis carried out to evaluate the comprehensivness and novelty of the risks generated by each focus group, for all uses.

First the distribution of the pair-wise BERTScore (bert-score library necessary) is explored through its quantiles, then once the threshold to consider a risk to be "matched" has been established, the overlap between generated risks and baseline (and vice versa) is computed, together with the tables for "common uses" and "overlooked uses" and desired visualizations.

```risks```

This folder contains all the risks generated by every focus group separated per use.

```Extract_risks_focus_group_plurals.ipynb```

This file contains the code that was used to simulate the focus groups from which risks associated to each use are generated.

Plurals library is necessary. To better understand the implementation of the different functionalities, refer to the Plurals documentation (https://josh-ashkinaze.github.io/plurals/) or the Plurals paper (https://arxiv.org/abs/2409.17213).

Plurals samples personas from the ANES database (https://electionstudies.org/data-center/), according to specified characteristics.
First we instantiate a "general" focus group of 100 people with random characteristics. Then we instantiate a "black women" focus group, comprised of 100 individuals from the ANES database with gender='woman' and race='black'. Finally, we instantiate a focus group with 100 people with oppressed characteristics, sampled following the distribution of people with at least two oppressed characteristics from the ANES database.

The focus groups discuss the possible risks of facial recognition technology (the use has been manually changed in the prompt, re-executing the cell every time and saving manually the results). Then a moderator is tasked to summarize the most relevant risks, providing examples from ExploreGen to adopt a similar (and therefore comparable) format (but specifying not to copy the content).

```evaluate_risks.py```
TBD

```calculate_accuracy.py```
Once we have both the labelling of human AI experts and the labelling of the LLM (for the subset of risks that we evaluate with Prolific), we can evaluate different metrics to validate the output of the LLM. For each label (question related to a risk) we calculate accuracy, adjacent accuracy (meaning that a value is considered to be correct if it falls between +-1 of the ground truth), MSE and RMSE.



